{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Basics of Image Classification Techniques in Machine Learning \n(https://iq.opengenus.org/basics-of-machine-learning-image-classification-techniques/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reading Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\nfrom pathlib import Path\nimport glob\nimport pandas as pd\nimport cv2\n\n# reading images from path\nimages_dir = Path('img')\nimages = images_dir.glob(\"*.tif\")\n\ntrain_data = []\n\ncounter = 0\nfor img in images:\n  counter += 1\n  if counter <= 130:\n    train_data.append((img,1))\n  else:\n    train_data.append((img,0))\n \n# converting data into pandas dataframe for easy visualization \ntrain_data = pd.DataFrame(train_data,columns=['image','label'],index = None)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Resize image"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.resize(img, (229,229))\n","execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'img' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-626c250c542a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m229\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m229\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"### Data Augmentation\n1. Gray Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'img' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ea8f4d640972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"2. Reflection/Flip"},{"metadata":{"trusted":true},"cell_type":"code","source":"# horizontal flip\nimg = cv2.flip(img, 0) \n\n# vertical flip\nimg = cv2.flip(img,1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Gaussian Blurring"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import ndimage\nimg = ndimage.gaussian_filter(img, sigma= 5.11)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Histogram Equalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram equalization function\ndef hist(img):\n  img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n  img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n  hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n  return hist_equalization_result\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Rotation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n# function for rotation\ndef rotation(img):\n  rows,cols = img.shape[0],img.shape[1]\n  randDeg = random.randint(-180, 180)\n  matrix = cv2.getRotationMatrix2D((cols/2, rows/2), randDeg, 0.70)\n  rotated = cv2.warpAffine(img, matrix, (rows, cols), borderMode=cv2.BORDER_CONSTANT,borderValue=(144, 159, 162))\n  return rotated     \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. Translation"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.warpAffine(img, np.float32([[1, 0, 84], [0, 1, 56]]), (img.shape[0], img.shape[1]),\nborderMode=cv2.BORDER_CONSTANT,borderValue=(144, 159, 162))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Classification Techniques\n#### 1. Support Vector Machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(16,(5,5),padding='valid',input_shape = X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.4))\nmodel.add(Conv2D(32,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.6))\nmodel.add(Conv2D(64,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Flatten())\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\nmodel_feat = Model(inputs=model.input,outputs=model.get_layer('dense_1').output)\nfeat_train = model_feat.predict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(kernel='rbf')\nsvm.fit(feat_train,np.argmax(y_train,axis=1))\n\nsvm.score(feat_test,np.argmax(y_test,axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(16,(5,5),padding='valid',input_shape = X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.4))\nmodel.add(Conv2D(32,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.6))\nmodel.add(Conv2D(64,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Flatten())\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\nmodel_feat = Model(inputs=model.input,outputs=model.get_layer('dense_2').output)\nfeat_train = model_feat.predict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth=3, min_samples_leaf=5)\ndt.fit(feat_train,np.argmax(y_train,axis=1))\n\ndt.score(feat_test,np.argmax(y_test,axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.K Nearest Neighbor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(16,(5,5),padding='valid',input_shape = X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.4))\nmodel.add(Conv2D(32,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.6))\nmodel.add(Conv2D(64,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Flatten())\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\nmodel_feat = Model(inputs=model.input,outputs=model.get_layer('dense_2').output)\nfeat_train = model_feat.predict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=12)\nknn.fit(feat_train,np.argmax(y_train,axis=-1))\n\nknn.score(feat_test,np.argmax(y_test,axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Artificial Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ann = Sequential()\nmodel_ann.add(Dense(16, input_shape=X_train.shape[1:], activation='relu'))\nmodel_ann.add(Dropout(0.4))\nmodel_ann.add(Dense(32, activation='relu'))\nmodel_ann.add(Dropout(0.6))\nmodel_ann.add(Flatten())\nmodel_ann.add(Dense(2, activation='softmax'))\n\nmodel_ann.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model_ann.fit(X_train, y_train,epochs=100,batch_size=100)\nhistory\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Convolutional Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(16,(5,5),padding='valid',input_shape = X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.4))\nmodel.add(Conv2D(32,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\nmodel.add(Dropout(0.6))\nmodel.add(Conv2D(64,(5,5),padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Flatten())\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\nbatch_size = 100\nepochs= 100\n\noptimizer = keras.optimizers.rmsprop(lr = 0.0001, decay = 1e-6)\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = optimizer, metrics = ['accuracy',keras_metrics.precision(), keras_metrics.recall()])\n\nhistory = model.fit(X_train,y_train,steps_per_epoch = int(len(X_train)/batch_size),epochs=epochs)\nhistory","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}