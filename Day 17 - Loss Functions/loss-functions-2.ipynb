{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How to Choose Loss Functions When Training Deep Learning Neural Networks\n(https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScale\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# generate regression dataset\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# standardize dataset\nX = StandardScaler().fit_transform(X)\ny = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# define model\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import SGD\nsgd = optimizers.SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='...', optimizer=sgd)\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Squared Error Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(1, activation='linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for regression with mse loss function\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\n# generate regression dataset\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n# standardize dataset\nX = StandardScaler().fit_transform(X)\ny = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='linear'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n# evaluate the model\ntrain_mse = model.evaluate(trainX, trainy, verbose=0)\ntest_mse = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n# plot loss during training\npyplot.title('Loss / Mean Squared Error')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Squared Logarithmic Error Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for regression with msle loss function\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\n# generate regression dataset\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n# standardize dataset\nX = StandardScaler().fit_transform(X)\ny = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='linear'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n# evaluate the model\n_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n_, test_mse = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot mse during training\npyplot.subplot(212)\npyplot.title('Mean Squared Error')\npyplot.plot(history.history['mean_squared_error'], label='train')\npyplot.plot(history.history['val_mean_squared_error'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Absolute Error Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for regression with mae loss function\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\n# generate regression dataset\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n# standardize dataset\nX = StandardScaler().fit_transform(X)\ny = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='linear'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n# evaluate the model\n_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n_, test_mse = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot mse during training\npyplot.subplot(212)\npyplot.title('Mean Squared Error')\npyplot.plot(history.history['mean_squared_error'], label='train')\npyplot.plot(history.history['val_mean_squared_error'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary Classification Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate circles\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot of the circles dataset with points colored by class\nfrom sklearn.datasets import make_circles\nfrom numpy import where\nfrom matplotlib import pyplot\n# generate circles\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n# select indices of points with each class label\nfor i in range(2):\n\tsamples_ix = where(y == i)\n\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='...'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='...', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary Cross-Entropy Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for the circles problem with cross entropy loss\nfrom sklearn.datasets import make_circles\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\n# generate 2d classification dataset\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='sigmoid'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hinge Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change y from {0,1} to {-1,1}\ny[where(y == 0)] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change y from {0,1} to {-1,1}\ny[where(y == 0)] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(1, activation='tanh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for the circles problem with hinge loss\nfrom sklearn.datasets import make_circles\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\nfrom numpy import where\n# generate 2d classification dataset\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n# change y from {0,1} to {-1,1}\ny[where(y == 0)] = -1\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='tanh'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Squared Hinge Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change y from {0,1} to {-1,1}\ny[where(y == 0)] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(1, activation='tanh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for the circles problem with squared hinge loss\nfrom sklearn.datasets import make_circles\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\nfrom numpy import where\n# generate 2d classification dataset\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n# change y from {0,1} to {-1,1}\ny[where(y == 0)] = -1\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='tanh'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multi-Class Classification Loss Functions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate dataset\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot of blobs dataset\nfrom sklearn.datasets import make_blobs\nfrom numpy import where\nfrom matplotlib import pyplot\n# generate dataset\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n# select indices of points with each class label\nfor i in range(3):\n\tsamples_ix = where(y == i)\n\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(..., activation='...'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile model\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='...', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multi-Class Cross-Entropy Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(3, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode output variable\ny = to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for the blobs multi-class classification problem with cross-entropy loss\nfrom sklearn.datasets import make_blobs\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot\n# generate 2d classification dataset\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n# one hot encode output variable\ny = to_categorical(y)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(3, activation='softmax'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sparse Multiclass Cross-Entropy Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(3, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for the blobs multi-class classification problem with sparse cross-entropy loss\nfrom sklearn.datasets import make_blobs\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom matplotlib import pyplot\n# generate 2d classification dataset\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(3, activation='softmax'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Kullback Leibler Divergence Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode output variable\ny = to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlp for the blobs multi-class classification problem with kl divergence loss\nfrom sklearn.datasets import make_blobs\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot\n# generate 2d classification dataset\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n# one hot encode output variable\ny = to_categorical(y)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]\n# define model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(3, activation='softmax'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}